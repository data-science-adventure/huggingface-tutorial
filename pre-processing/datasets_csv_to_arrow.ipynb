{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68ee5f4e",
   "metadata": {},
   "source": [
    "# CSV to ClassLabel to Apache Arrow Splits\n",
    "\n",
    "This solution uses `load_dataset`, `.cast()`, `.train_test_split()`, and `save_to_disk()` to achieve the final, optimized result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40574a",
   "metadata": {},
   "source": [
    "### Prerequisites and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f3c9beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/projects/git/organizations/data-science-adventure/huggingface-tutorial/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, DatasetDict, Features, ClassLabel, load_from_disk\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "CSV_FILE_NAME = \"../datasets/product_reviews.csv\"\n",
    "FINAL_SAVE_PATH = \"../product_review_splits\"\n",
    "SEED = 40 # For reproducible splits\n",
    "TEST_SIZE = 0.10\n",
    "VALID_SIZE = 0.1111 # 10% of the original data is ~11.11% of the remaining 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814b8540",
   "metadata": {},
   "source": [
    "### 1. Load CSV and Cast to ClassLabel\n",
    "\n",
    "We load the CSV and immediately define and apply the ClassLabel feature to the sentiment column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024b5622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100 examples [00:00, 35025.50 examples/s]\n",
      "Casting the dataset: 100%|██████████| 100/100 [00:00<00:00, 44892.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- After Casting ---\n",
      "Dataset features: {'product': Value('string'), 'review': Value('string'), 'sentiment': ClassLabel(names=['negative', 'neutral', 'positive'])}\n",
      "First example label (integer ID): 2\n",
      "Decoded label: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the CSV. 'sentiment' loads as a simple string.\n",
    "initial_dataset_dict = load_dataset(\"csv\", data_files=CSV_FILE_NAME)\n",
    "raw_dataset = initial_dataset_dict[\"train\"] # Access the single 'train' split\n",
    "\n",
    "# 3. Define the ClassLabel Feature\n",
    "# The order here defines the integer mapping: negative=0, neutral=1, positive=2\n",
    "sentiment_features = raw_dataset.features.copy()\n",
    "sentiment_features[\"sentiment\"] = ClassLabel(names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "# 4. Cast the column\n",
    "processed_dataset = raw_dataset.cast(sentiment_features)\n",
    "\n",
    "print(\"\\n--- After Casting ---\")\n",
    "print(f\"Dataset features: {processed_dataset.features}\")\n",
    "print(f\"First example label (integer ID): {processed_dataset[0]['sentiment']}\")\n",
    "print(f\"Decoded label: {processed_dataset.features['sentiment'].int2str(processed_dataset[0]['sentiment'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543a573",
   "metadata": {},
   "source": [
    "### 2. Split into Train, Validation, and Test Sets\n",
    "\n",
    "We apply the **two-step split** process to partition the data into the three required splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f08a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split Sizes ---\n",
      "Train: 80 examples\n",
      "Validation: 10 examples\n",
      "Test: 10 examples\n"
     ]
    }
   ],
   "source": [
    "# 5. First Split: Separate Test Set (10%)\n",
    "# Splits the data into a Test set (10%) and a larger Train/Valid set (90%)\n",
    "train_valid_splits = processed_dataset.train_test_split(test_size=TEST_SIZE, seed=SEED)\n",
    "\n",
    "test_dataset = train_valid_splits[\"test\"]\n",
    "train_valid_dataset = train_valid_splits[\"train\"]\n",
    "\n",
    "# 6. Second Split: Separate Train and Validation Sets\n",
    "# Splits the remaining 90% into Train (80%) and Validation (10%)\n",
    "# VALID_SIZE = 0.1111 (10% / 90%)\n",
    "final_splits = train_valid_dataset.train_test_split(test_size=VALID_SIZE, seed=SEED)\n",
    "\n",
    "train_dataset = final_splits[\"train\"]\n",
    "valid_dataset = final_splits[\"test\"] # The 'test' of this split becomes our validation\n",
    "\n",
    "print(\"\\n--- Split Sizes ---\")\n",
    "print(f\"Train: {len(train_dataset)} examples\")\n",
    "print(f\"Validation: {len(valid_dataset)} examples\")\n",
    "print(f\"Test: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dc114d",
   "metadata": {},
   "source": [
    "### 3. Save as Apache Arrow Format\n",
    "\n",
    "We assemble the three splits into a `DatasetDict` and use `save_to_disk()` (which uses the Apache Arrow/Parquet format by default) as the best practice for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc49756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 80/80 [00:00<00:00, 29826.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 4147.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 4223.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved all splits in Apache Arrow format to: ../product_review_splits\n",
      "Saved DatasetDict structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['product', 'review', 'sentiment'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['product', 'review', 'sentiment'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['product', 'review', 'sentiment'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Combine into a DatasetDict\n",
    "final_dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": valid_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "\n",
    "# 8. Save the entire DatasetDict to disk (Best Practice)\n",
    "final_dataset_dict.save_to_disk(FINAL_SAVE_PATH)\n",
    "\n",
    "print(f\"\\nSuccessfully saved all splits in Apache Arrow format to: {FINAL_SAVE_PATH}\")\n",
    "print(f\"Saved DatasetDict structure:\\n{final_dataset_dict}\")\n",
    "\n",
    "# Clean up the intermediate CSV\n",
    "#os.remove(CSV_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf87631",
   "metadata": {},
   "source": [
    "### 4. Loading the Final Dataset\n",
    "\n",
    "To demonstrate that the saved files retain the `ClassLabel` metadata, we load the data using `load_from_disk()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1d8eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification: Reloaded Features ---\n",
      "Loaded validation set features:\n",
      "ClassLabel(names=['negative', 'neutral', 'positive'])\n",
      "Loaded validation set size: 10\n",
      "\n",
      "--- DatasetDict\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['product', 'review', 'sentiment'],\n",
      "        num_rows: 80\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['product', 'review', 'sentiment'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['product', 'review', 'sentiment'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 9. Load the DatasetDict from the saved directory\n",
    "loaded_splits = load_from_disk(FINAL_SAVE_PATH)\n",
    "\n",
    "# Verify the features are correct\n",
    "print(\"\\n--- Verification: Reloaded Features ---\")\n",
    "print(f\"Loaded validation set features:\\n{loaded_splits['validation'].features['sentiment']}\")\n",
    "print(f\"Loaded validation set size: {len(loaded_splits['validation'])}\")\n",
    "print(\"\\n--- DatasetDict\")\n",
    "print(loaded_splits)\n",
    "# Clean up the saved directory\n",
    "#shutil.rmtree(FINAL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
