{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd653b5",
   "metadata": {},
   "source": [
    "# Overview of the Hugging Face datasets Library\n",
    "The Hugging Face **datasets** library provides a uniform, efficient, and lightweight way to access, process, and share datasets for NLP, as well as Computer Vision and Audio tasks.\n",
    "\n",
    "## Key Classes\n",
    "\n",
    "**DatasetDict**: A dictionary-like object that holds multiple Dataset objects, typically the different splits (e.g., `train`, `validation`, `test`).This is what _load_dataset_ often returns.\n",
    "\n",
    "**Dataset**: The main object, similar to a `pandas.DataFrame`, but optimized for ML workflows. It provides dictionary-style access to rows and columns.\n",
    "\n",
    "**Features**: Defines the data types and structure of your dataset columns (e.g., `Value('string')`, `ClassLabel` for labels, or specialized features for `Image` and `Audio`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbc749b",
   "metadata": {},
   "source": [
    "## How to Use the Hugging Face datasets Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d433b",
   "metadata": {},
   "source": [
    "### Step 1: Installation\n",
    "\n",
    "```shell\n",
    "pip install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a9eeca",
   "metadata": {},
   "source": [
    "### Step 2: Loading Data\n",
    "\n",
    "#### A. Loading from the Hugging Face Hub (Public Datasets)\n",
    "\n",
    "This is the most common use case. You simply provide the dataset identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2e4ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/projects/git/organizations/data-science-adventure/information-extraction/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 25000\n",
      "Test split size: 25000\n",
      "{'text': Value('string'), 'label': ClassLabel(names=['neg', 'pos'])}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the entire dataset dictionary for the IMDB review classification dataset\n",
    "# It will download and cache the data.\n",
    "dataset_dict = load_dataset(\"imdb\")\n",
    "\n",
    "# Access the individual splits\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "test_dataset = dataset_dict[\"test\"]\n",
    "\n",
    "print(f\"Train split size: {len(train_dataset)}\")\n",
    "print(f\"Test split size: {len(test_dataset)}\")\n",
    "print(train_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7161fe",
   "metadata": {},
   "source": [
    "#### B. Loading from Local Files (CSV, JSON, Text, etc.)\n",
    "\n",
    "You specify the data format (e.g., 'csv', 'json') and provide the path to your file(s).21Assume you have a local file named `my_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7493fef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3 examples [00:00, 768.56 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n",
      "{'text': 'This is great.', 'label': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dummy CSV file for the example\n",
    "pd.DataFrame({\n",
    "    'text': [\"This is great.\", \"This is terrible.\", \"So-so.\"],\n",
    "    'label': [1, 0, 1]\n",
    "}).to_csv(\"my_data.csv\", index=False)\n",
    "\n",
    "# Load the local CSV file\n",
    "local_dataset = load_dataset(\"csv\", data_files=\"my_data.csv\")\n",
    "\n",
    "print(local_dataset)\n",
    "print(local_dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b042c",
   "metadata": {},
   "source": [
    "##### How to Load `ClassLabel` from CSV\n",
    "\n",
    "To load a label column (e.g., `'positive'`, `'negative'`) from a local CSV as a dedicated **`ClassLabel`** feature, you must explicitly **cast** the column's type after the initial load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193e0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3 examples [00:00, 886.81 examples/s]\n",
      "Casting the dataset: 100%|██████████| 3/3 [00:00<00:00, 1165.30 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ClassLabel Conversion Complete ---\n",
      "Final Feature Type: ClassLabel(names=['negative', 'neutral', 'positive'])\n",
      "Example ID: 0, Decoded Label: negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, ClassLabel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create dummy data for the example\n",
    "df = pd.DataFrame({\n",
    "    'text': [\"This is great!\", \"I'm not happy.\", \"Neutral feeling.\"],\n",
    "    'sentiment': [\"positive\", \"negative\", \"neutral\"]\n",
    "})\n",
    "df.to_csv(\"sentiment_data.csv\", index=False)\n",
    "\n",
    "# 1. Initial Load: 'sentiment' loads as a simple string Value\n",
    "local_data = load_dataset(\"csv\", data_files=\"sentiment_data.csv\")\n",
    "raw_dataset = local_data[\"train\"]\n",
    "\n",
    "# 2. Define the Target Features\n",
    "# IMPORTANT: The 'names' list defines the string-to-integer mapping (0, 1, 2)\n",
    "new_features = raw_dataset.features.copy()\n",
    "new_features[\"sentiment\"] = ClassLabel(names=['negative', 'neutral', 'positive'])\n",
    "\n",
    "# 3. Cast the Dataset\n",
    "# This converts the string labels into their corresponding integer IDs (0, 1, or 2)\n",
    "dataset_with_classlabel = raw_dataset.cast(new_features)\n",
    "\n",
    "print(\"\\n--- ClassLabel Conversion Complete ---\")\n",
    "print(f\"Final Feature Type: {dataset_with_classlabel.features['sentiment']}\")\n",
    "\n",
    "# Demonstrating label decoding:\n",
    "example_id = dataset_with_classlabel[1]['sentiment']\n",
    "example_label_name = dataset_with_classlabel.features['sentiment'].int2str(example_id)\n",
    "\n",
    "print(f\"Example ID: {example_id}, Decoded Label: {example_label_name}\")\n",
    "\n",
    "# Clean up the dummy file\n",
    "os.remove(\"sentiment_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace5cbf",
   "metadata": {},
   "source": [
    "##### Saving the Processed Dataset\n",
    "\n",
    "After you have loaded your initial CSV and successfully cast the column to `ClassLabel`, you should save the resulting `Dataset` object.\n",
    "\n",
    "**1. The Best Practice Code**\n",
    "\n",
    "This process saves the data, the schema, and the `ClassLabel` mapping to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e2ee0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3/3 [00:00<00:00, 1045.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully saved to: ./my_processed_sentiment_dataset\n",
      "This directory now contains the data and the feature schema.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import os\n",
    "\n",
    "# Assume 'dataset_with_classlabel' is the Dataset object you created in the last step\n",
    "# with the 'sentiment' column already cast to ClassLabel.\n",
    "\n",
    "# Define the path where the dataset structure will be saved\n",
    "SAVE_PATH = \"./my_processed_sentiment_dataset\"\n",
    "\n",
    "# --- Save the Dataset ---\n",
    "dataset_with_classlabel.save_to_disk(SAVE_PATH)\n",
    "\n",
    "print(f\"Dataset successfully saved to: {SAVE_PATH}\")\n",
    "print(\"This directory now contains the data and the feature schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82541946",
   "metadata": {},
   "source": [
    "**2. Loading the Dataset with Features Intact**\n",
    "\n",
    "To load the dataset later, you use the `load_from_disk()` function, which instantly re-reads the data, including all the feature definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1baf0a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Reloaded Dataset Check ---\n",
      "Features upon reload: ClassLabel(names=['negative', 'neutral', 'positive'])\n",
      "Example value: 2\n",
      "Decoded label: positive\n"
     ]
    }
   ],
   "source": [
    "# --- Load the Dataset ---\n",
    "reloaded_dataset = load_from_disk(SAVE_PATH)\n",
    "\n",
    "print(\"\\n--- Reloaded Dataset Check ---\")\n",
    "print(f\"Features upon reload: {reloaded_dataset.features['sentiment']}\")\n",
    "print(f\"Example value: {reloaded_dataset[0]['sentiment']}\")\n",
    "print(f\"Decoded label: {reloaded_dataset.features['sentiment'].int2str(reloaded_dataset[0]['sentiment'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b3c51",
   "metadata": {},
   "source": [
    "**Why this is the best practice**\n",
    "\n",
    "1. **Preserves Metadata:** When you save to disk using `save_to_disk()`, the entire **schema**, including your custom `ClassLabel` definition, is serialized and saved alongside the data files (in Arrow/Parquet format).\n",
    "2. **Instant Loading:** `load_from_disk()` is typically **faster** than re-loading from a generic format like CSV, as it skips file parsing and schema inference steps, directly loading the optimized columnar structure.\n",
    "3. **Efficiency:** It ensures the dataset is in the most **optimized columnar format** (Apache Arrow/Parquet) for subsequent mapping and processing operations, avoiding the performance bottlenecks of plain text files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da38871",
   "metadata": {},
   "source": [
    "#### C. Creating a Dataset from a Python Object\n",
    "\n",
    "You can easily convert standard Python lists or dictionaries into a **Dataset** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39740aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sentence', 'id'],\n",
      "    num_rows: 2\n",
      "})\n",
      "Column(['Hello, world!', 'Coding is fun.'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset from a dictionary\n",
    "data = {\n",
    "    \"sentence\": [\"Hello, world!\", \"Coding is fun.\"],\n",
    "    \"id\": [1, 2]\n",
    "}\n",
    "my_dataset = Dataset.from_dict(data)\n",
    "\n",
    "print(my_dataset)\n",
    "print(my_dataset[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a40f6e",
   "metadata": {},
   "source": [
    "## Data Manipulation Examples\n",
    "\n",
    "The **Dataset** object has a powerful set of methods for preprocessing, which are applied efficiently using Apache Arrow.\n",
    "\n",
    "### 1. Tokenization with `map()`\n",
    "\n",
    "The `map()` method is the workhorse of data processing. It applies a function to every example (or batch of examples) in the dataset. This is where you would typically perform tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51d1e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokenized Dataset ---\n",
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 500\n",
      "})\n",
      "{'labels': 1, 'input_ids': [101, 2572, 3217, 5831, 5496, 2010, 2567, 1010, 3183, 2002, 2170, 1000, 1996, 7409, 1000, 1010, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102, 7727, 2000, 2032, 2004, 2069, 1000, 1996, 7409, 1000, 1010, 2572, 3217, 5831, 5496, 2010, 2567, 1997, 9969, 4487, 23809, 3436, 2010, 3350, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a small dataset split\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\", split=\"train[:500]\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # This processes two sentences, as in a sentence-pair classification task\n",
    "    return tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Apply the function to the entire dataset\n",
    "# batched=True is often faster and necessary for some tokenizers\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove original text columns and rename 'label' to 'labels' for Transformer model compatibility\n",
    "final_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "final_datasets = final_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(\"--- Tokenized Dataset ---\")\n",
    "print(final_datasets)\n",
    "print(final_datasets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54f12f",
   "metadata": {},
   "source": [
    "### 2. Filtering Examples\n",
    "\n",
    "The `filter()` method keeps examples that satisfy a condition defined in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6cc359c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Filtered Dataset (Positive Reviews) ---\n",
      "Original size: 500, Positive size: 346\n"
     ]
    }
   ],
   "source": [
    "# Filter the IMDB dataset to only keep positive reviews (label 1)\n",
    "def is_positive(example):\n",
    "    # The 'label' feature has ClassLabel, where 1 is positive\n",
    "    return example[\"label\"] == 1\n",
    "\n",
    "positive_reviews = raw_datasets.filter(is_positive)\n",
    "\n",
    "print(\"\\n--- Filtered Dataset (Positive Reviews) ---\")\n",
    "print(f\"Original size: {len(raw_datasets)}, Positive size: {len(positive_reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790d30f",
   "metadata": {},
   "source": [
    "### 3. Selecting Columns and Rows\n",
    "\n",
    "You can use familiar Python/NumPy slicing and indexing.\n",
    "\n",
    "\n",
    "| **Method**             | **Description**                           | **Example**                                  |\n",
    "| ---------------------- | ----------------------------------------- | -------------------------------------------- |\n",
    "| **Indexing/Slicing**   | Accesses a single row or a range of rows. | `train_dataset[0]`                 |\n",
    "| **Column Access**      | Accesses an entire column.                | `train_dataset['text']`            |\n",
    "| **`select()`**         | Selects examples by a list of indices.    | `dataset.select(range(100))`        |\n",
    "| **`remove_columns()`** | Removes one or more columns.              | `dataset.remove_columns([\"text\"])` |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b01c6",
   "metadata": {},
   "source": [
    "### 4. Splitting and Shuffling\n",
    "\n",
    "You can easily split a single split into training and testing portions using a simple method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c6a8fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Split DatasetDict ---\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 450\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n",
      "New validation split size: 50\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into new train and validation splits\n",
    "split_datasets = raw_datasets.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(\"\\n--- Split DatasetDict ---\")\n",
    "print(split_datasets)\n",
    "print(f\"New validation split size: {len(split_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993ef54",
   "metadata": {},
   "source": [
    "### 5. Formatting for Frameworks\n",
    "\n",
    "To prepare the dataset for direct use in PyTorch, TensorFlow, or NumPy, you use `set_format()`. This automatically converts the Apache Arrow arrays into the correct tensor format and makes columns required by the framework (like `input_ids`) available as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e941d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PyTorch Dataset ---\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# Set the format to PyTorch tensors\n",
    "pytorch_dataset = final_datasets.with_format(\"torch\")\n",
    "\n",
    "print(\"\\n--- PyTorch Dataset ---\")\n",
    "# The columns are now PyTorch tensors when accessed\n",
    "print(type(pytorch_dataset[0]['input_ids']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
